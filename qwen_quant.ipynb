{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsxr48gwMper",
        "outputId": "948b26cd-6365-42bf-a5b6-0401c06c352f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl size=5959644 sha256=d2f85c008b5f4d4d3788b4f64e0d28a989a7d4a49fb3a9523e2e1404eee1912f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/03/66/eb3810eafd55d921b2be32896d1f44313996982360663aa80b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install llama-cpp-python\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCh6rKDKOQps",
        "outputId": "cf3f5438-92f2-42a6-a929-6cae21f261fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "model_path = \"/content/drive/My Drive/Qwen2-500M-Instruct-Q4_K_M.gguf\"\n",
        "llm = Llama(model_path=model_path)\n",
        "\n",
        "output = llm(\"Q: What is the capital of France? A:\")\n",
        "print(\"output is :\" ,output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwYK4inVO-YD",
        "outputId": "b8d2c8cb-5381-4611-f92b-6f1dd35f2994"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 25 key-value pairs and 290 tensors from /content/drive/My Drive/Qwen2-500M-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Qwen2-0.5B-Instruct\n",
            "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
            "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\n",
            "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\n",
            "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\n",
            "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                      quantize.imatrix.file str              = /models/Qwen2-0.5B-Instruct-GGUF/Qwen...\n",
            "llama_model_loader: - kv  22:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
            "llama_model_loader: - kv  23:             quantize.imatrix.entries_count i32              = 168\n",
            "llama_model_loader: - kv  24:              quantize.imatrix.chunks_count i32              = 128\n",
            "llama_model_loader: - type  f32:  121 tensors\n",
            "llama_model_loader: - type q5_0:  132 tensors\n",
            "llama_model_loader: - type q8_0:   13 tensors\n",
            "llama_model_loader: - type q4_K:   12 tensors\n",
            "llama_model_loader: - type q6_K:   12 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 373.71 MiB (6.35 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: special tokens cache size = 293\n",
            "load: token to piece cache size = 0.9338 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 896\n",
            "print_info: n_layer          = 24\n",
            "print_info: n_head           = 14\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = 7\n",
            "print_info: n_embd_k_gqa     = 128\n",
            "print_info: n_embd_v_gqa     = 128\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 4864\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 1B\n",
            "print_info: model params     = 494.03 M\n",
            "print_info: general.name     = Qwen2-0.5B-Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =   373.71 MiB\n",
            ".................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 1000000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init:        CPU KV buffer size =     6.00 MiB\n",
            "llama_init_from_model: KV self size  =    6.00 MiB, K (f16):    3.00 MiB, V (f16):    3.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   298.50 MiB\n",
            "llama_init_from_model: graph nodes  = 846\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '168', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.file': '/models/Qwen2-0.5B-Instruct-GGUF/Qwen2-0.5B-Instruct.imatrix', 'tokenizer.ggml.bos_token_id': '151643', 'general.architecture': 'qwen2', 'qwen2.block_count': '24', 'qwen2.context_length': '32768', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'qwen2.attention.head_count_kv': '2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '896', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.attention.head_count': '14', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.file_type': '15', 'general.quantization_version': '2', 'qwen2.feed_forward_length': '4864', 'tokenizer.ggml.model': 'gpt2', 'general.name': 'Qwen2-0.5B-Instruct', 'tokenizer.ggml.pre': 'qwen2'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n",
            "llama_perf_context_print:        load time =     475.76 ms\n",
            "llama_perf_context_print: prompt eval time =     475.53 ms /    11 tokens (   43.23 ms per token,    23.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1100.74 ms /    15 runs   (   73.38 ms per token,    13.63 tokens per second)\n",
            "llama_perf_context_print:       total time =    1603.39 ms /    26 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output is : {'id': 'cmpl-4efbf8a2-b005-4aaa-8e72-8e51da317b75', 'object': 'text_completion', 'created': 1742703829, 'model': '/content/drive/My Drive/Qwen2-500M-Instruct-Q4_K_M.gguf', 'choices': [{'text': ' Luxembourg\\nAnswer: No, the capital of France is Paris. Luxembourg is the', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 11, 'completion_tokens': 16, 'total_tokens': 27}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqfBhJ6nQaGD",
        "outputId": "16d76107-342e-4947-cae0-10fa3943c603"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from llama_cpp import Llama\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "svamp = load_dataset(\"ChilleD/SVAMP\")\n",
        "\n",
        "#model_path = \"/content/drive/My Drive/Qwen2-500M-Instruct-Q4_K_M.gguf\"\n",
        "#model_path = \"/content/drive/My Drive/Qwen2-500M-Instruct-Q8_0.gguf\"\n",
        "#model_path = \"/content/drive/My Drive/Qwen2-500M-Instruct-f32.gguf\"\n",
        "#model_path = \"/content/drive/My Drive/Qwen2-500M-Instruct-f32.gguf\"\n",
        "#model_path = \"/content/drive/My Drive/Qwen2-1.5B.Q2_K.gguf\"\n",
        "model_path = \"/content/drive/My Drive/Qwen2-1.5B.Q8_0.gguf\"\n",
        "llm = Llama(model_path=model_path)\n",
        "\n",
        "client = OpenAI(api_key=\"not uploading because of security issues\")\n",
        "\n",
        "\n",
        "correct, total = 0, 20\n",
        "\n",
        "start_time = time.time()\n",
        "for example in svamp['test'].select(range(total)):\n",
        "    question_text = example['Body'] + \" \" + example['Question']\n",
        "    correct_answer = str(example['Answer'])\n",
        "    prompt = f\"<|im_start|>user\\n{question_text}\\nLet's think step by step.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "    tokens = llm.tokenize(prompt.encode(\"utf-8\"))\n",
        "    print(f\"Tokenized input ({len(tokens)} tokens): {tokens[:20]} ...\")\n",
        "\n",
        "    response = llm(\n",
        "        prompt,\n",
        "        max_tokens=256,\n",
        "        temperature=0,\n",
        "        stop=[\"<|im_end|>\"]\n",
        "    )\n",
        "\n",
        "    full_response = response['choices'][0]['text'].strip()\n",
        "\n",
        "    verification_prompt = f\"\"\"Question: {question_text} Model Response: {full_response} Correct Answer: {correct_answer} Based on the logical analysis of the Model Response and the Correct Answer provided, is the model's response correct? Reply ONLY with a single word: TRUE or FALSE.\"\"\"\n",
        "\n",
        "    chatgpt_response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": verification_prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    verification_result = chatgpt_response.choices[0].message.content.strip().upper()\n",
        "\n",
        "    print(f\"Question: {question_text}\")\n",
        "    print(f\"Model Response:\\n{full_response}\")\n",
        "    print(f\"ChatGPT Verification: {verification_result}\")\n",
        "\n",
        "    if verification_result == \"TRUE\":\n",
        "        correct += 1\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f}s\")\n",
        "\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy on SVAMP dataset: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa7z-8P1QPsE",
        "outputId": "c75ad42b-776a-4475-8fd3-9106c16e22f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 338 tensors from /content/drive/My Drive/Qwen2-1.5B.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.name str              = models\n",
            "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q8_0:  197 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 1.53 GiB (8.50 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: special tokens cache size = 293\n",
            "load: token to piece cache size = 0.9338 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 1536\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 6\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8960\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 1.5B\n",
            "print_info: model params     = 1.54 B\n",
            "print_info: general.name     = models\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  1564.62 MiB\n",
            "......................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 1000000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init:        CPU KV buffer size =    14.00 MiB\n",
            "llama_init_from_model: KV self size  =   14.00 MiB, K (f16):    7.00 MiB, V (f16):    7.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   299.75 MiB\n",
            "llama_init_from_model: graph nodes  = 986\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.bos_token_id': '151643', 'general.architecture': 'qwen2', 'qwen2.block_count': '28', 'qwen2.context_length': '131072', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'qwen2.attention.head_count_kv': '2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '1536', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.attention.head_count': '12', 'tokenizer.ggml.eos_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'general.file_type': '7', 'general.quantization_version': '2', 'qwen2.feed_forward_length': '8960', 'tokenizer.ggml.model': 'gpt2', 'general.name': 'models', 'tokenizer.ggml.pre': 'qwen2'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
            "You are a helpful assistant<|im_end|>\n",
            "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized input (90 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 66188, 374, 4558, 1588, 323, 1429, 9898, 525, 84526, 311, 44939, 5837] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    6419.81 ms /    77 tokens (   83.37 ms per token,    11.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =   26957.96 ms /   111 runs   (  242.86 ms per token,     4.12 tokens per second)\n",
            "llama_perf_context_print:       total time =   33591.46 ms /   188 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Winter is almost here and most animals are migrating to warmer countries. There are 41 bird families living near the mountain. If 35 bird families flew away to asia and 62 bird families flew away to africa How many more bird families flew away to africa than those that flew away to asia?\n",
            "Model Response:\n",
            "Based on the information provided, we can set up the following equation:\n",
            "\n",
            "35 bird families flew away to Asia\n",
            "62 bird families flew away to Africa\n",
            "\n",
            "We want to find out how many more bird families flew away to Africa than those that flew away to Asia. So we can subtract the number of bird families that flew away to Asia from the number of bird families that flew away to Africa:\n",
            "\n",
            "62 - 35 = 27\n",
            "\n",
            "So, 27 more bird families flew away to Africa than those that flew away to Asia.\n",
            "ChatGPT Verification: TRUE\n",
            "Tokenized input (64 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 20291, 7256, 9226, 220, 22, 6623, 18170, 323, 220, 16, 17, 8251] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 48 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    3017.45 ms /    48 tokens (   62.86 ms per token,    15.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =   16616.97 ms /    70 runs   (  237.39 ms per token,     4.21 tokens per second)\n",
            "llama_perf_context_print:       total time =   19764.29 ms /   118 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Paige raised 7 goldfish and 12 catfish in the pond but stray cats loved eating them. Now she has 15 left. How many fishes disappeared?\n",
            "Model Response:\n",
            "Based on the information provided, Paige started with 7 goldfish and 12 catfish, which totals 19 fishes. After stray cats ate some, she was left with 15 fishes. So, the number of fishes that disappeared is:\n",
            "\n",
            "19 - 15 = 4 fishes\n",
            "\n",
            "So, 4 fishes disappeared.\n",
            "ChatGPT Verification: TRUE\n",
            "Tokenized input (83 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 85384, 323, 806, 17760, 3937, 72600, 21132, 13, 31155, 807, 14548, 75103] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 67 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    6780.12 ms /    67 tokens (  101.20 ms per token,     9.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =   27145.85 ms /   111 runs   (  244.56 ms per token,     4.09 tokens per second)\n",
            "llama_perf_context_print:       total time =   34137.78 ms /   178 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Marco and his dad went strawberry picking. Together they collected strawberries that weighed 22 pounds. On the way back Marco ' dad found 30 more pounds of strawberries. Marco's strawberries now weighed 36 pounds. How much did his dad's strawberries weigh now?\n",
            "Model Response:\n",
            "Based on the information given, we can set up the following equation to find the weight of Marco's dad's strawberries:\n",
            "\n",
            "Marco's strawberries + 30 pounds = Marco's dad's strawberries\n",
            "\n",
            "We know that Marco's strawberries weighed 22 pounds and his dad found 30 more pounds, so we can write:\n",
            "\n",
            "22 pounds + 30 pounds = Marco's dad's strawberries\n",
            "\n",
            "Combining like terms, we get:\n",
            "\n",
            "52 pounds = Marco's dad's strawberries\n",
            "\n",
            "So, Marco's dad's strawberries weighed 52 pounds.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (76 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 1912, 65, 1694, 10788, 220, 17, 15, 15, 3015, 26376, 323, 220] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 60 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    3745.53 ms /    60 tokens (   62.43 ms per token,    16.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =   25302.10 ms /   104 runs   (  243.29 ms per token,     4.11 tokens per second)\n",
            "llama_perf_context_print:       total time =   29245.10 ms /   164 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Debby bought 200 water bottles and 256 soda bottles when they were on sale. If she drank 312 water bottles and 4 soda bottles a day How many days would the soda bottles last?\n",
            "Model Response:\n",
            "Based on the information provided, we can set up the following equation to find out how many days the soda bottles would last:\n",
            "\n",
            "Number of soda bottles * Number of days = Total number of soda bottles\n",
            "\n",
            "256 * Number of days = 4 * 312\n",
            "\n",
            "Now we can solve for the number of days:\n",
            "\n",
            "Number of days = (4 * 312) / 256\n",
            "\n",
            "Number of days = 12\n",
            "\n",
            "So the soda bottles would last for 12 days.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (75 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 3862, 1033, 220, 16, 15, 21, 11192, 304, 59287, 594, 15085, 13] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 59 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    3731.01 ms /    59 tokens (   63.24 ms per token,    15.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =   39609.36 ms /   164 runs   (  241.52 ms per token,     4.14 tokens per second)\n",
            "llama_perf_context_print:       total time =   43665.67 ms /   223 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: There were 106 dollars in Olivia's wallet. After she visited a supermarket and a showroom there were 26 dollars left. If she spent 49 dollars at the showroom How much did she spend at the supermarket?\n",
            "Model Response:\n",
            "To find out how much Olivia spent at the supermarket, we can use the following steps:\n",
            "\n",
            "1. We know that Olivia had 106 dollars in her wallet initially.\n",
            "2. After visiting the supermarket, she had 26 dollars left.\n",
            "3. We are told that she spent 49 dollars at the showroom.\n",
            "\n",
            "So, we can set up the equation:\n",
            "\n",
            "Initial amount - Amount spent at supermarket - Amount spent at showroom = Amount left\n",
            "\n",
            "106 - 49 - 26 = 26\n",
            "\n",
            "Solving for the amount spent at the supermarket:\n",
            "\n",
            "106 - 49 - 26 = 26\n",
            "106 - 75 = 26\n",
            "31 = 26\n",
            "\n",
            "So, Olivia spent 26 dollars at the supermarket.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (64 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 37, 59955, 572, 24047, 1059, 96338, 323, 88916, 2382, 1119, 220, 18] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 48 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    3063.36 ms /    48 tokens (   63.82 ms per token,    15.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =   29145.29 ms /   122 runs   (  238.90 ms per token,     4.19 tokens per second)\n",
            "llama_perf_context_print:       total time =   32443.45 ms /   170 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Faye was placing her pencils and crayons into 30 rows with 71 crayons and 24 pencils in each row. How many pencils does she have?\n",
            "Model Response:\n",
            "To find the number of pencils Faye has, we can use the information given about the number of crayons and pencils in each row.\n",
            "\n",
            "We know that there are 71 crayons in each row and 24 pencils in each row. So, the total number of crayons and pencils in each row is 71 + 24 = 95.\n",
            "\n",
            "Since there are 30 rows, the total number of crayons and pencils in all rows is 95 * 30 = 2850.\n",
            "\n",
            "Therefore, Faye has 2850 pencils.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (72 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 2715, 1030, 220, 16, 15, 803, 3594, 38763, 1091, 31508, 13, 31508] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    5959.24 ms /    56 tokens (  106.42 ms per token,     9.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =   15166.53 ms /    64 runs   (  236.98 ms per token,     4.22 tokens per second)\n",
            "llama_perf_context_print:       total time =   21245.45 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Ed had 10 more marbles than Doug. Doug lost 11 of his marbles at the playground. If Ed had 45 marbles How many more marbles did Ed have than Doug then?\n",
            "Model Response:\n",
            "Ed had 10 more marbles than Doug initially. After Doug lost 11 marbles, Ed had 45 marbles. So, Ed had 45 - 11 = 34 marbles more than Doug initially. Therefore, Ed had 34 more marbles than Doug.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (60 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 17, 21, 2841, 1033, 19837, 389, 279, 5828, 13, 2411, 279, 5828] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 44 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    2728.20 ms /    44 tokens (   62.00 ms per token,    16.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =   21410.10 ms /    90 runs   (  237.89 ms per token,     4.20 tokens per second)\n",
            "llama_perf_context_print:       total time =   24304.57 ms /   134 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: 26 children were riding on the bus. At the bus stop 38 more children got on the bus. How many children are on the bus now?\n",
            "Model Response:\n",
            "To find out how many children are on the bus now, we can use the following steps:\n",
            "\n",
            "1. Start with the initial number of children on the bus: 26.\n",
            "2. Add the number of children who got on the bus at the bus stop: 38.\n",
            "3. Add the two numbers together: 26 + 38 = 64.\n",
            "\n",
            "So, there are now 64 children on the bus.\n",
            "ChatGPT Verification: TRUE\n",
            "Tokenized input (81 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 39, 8398, 279, 43556, 78, 323, 1059, 4780, 525, 20045, 369, 9702] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 65 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    5277.46 ms /    65 tokens (   81.19 ms per token,    12.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =   18325.83 ms /    78 runs   (  234.95 ms per token,     4.26 tokens per second)\n",
            "llama_perf_context_print:       total time =   23747.99 ms /   143 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Helen the hippo and her friends are preparing for thanksgiving at Helen's house. Helen baked 31 cookies yesterday 270 cookies this morning and 419 cookies the day before yesterday. How many cookies did Helen bake till last night?\n",
            "Model Response:\n",
            "Helen baked 31 cookies yesterday, 270 cookies this morning, and 419 cookies the day before yesterday. To find out how many cookies she baked till last night, we need to add these amounts together:\n",
            "\n",
            "31 + 270 + 419 = 720\n",
            "\n",
            "Helen baked 720 cookies till last night.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (75 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 1519, 1221, 3937, 311, 1490, 279, 84038, 1660, 66472, 13, 1260, 1730] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 59 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    6126.93 ms /    59 tokens (  103.85 ms per token,     9.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =   33921.09 ms /   142 runs   (  238.88 ms per token,     4.19 tokens per second)\n",
            "llama_perf_context_print:       total time =   40326.35 ms /   201 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: He then went to see the oranges being harvested. He found out that they harvest 66 sacks per day and that each sack containes 25 oranges. How many oranges will they have after 87 days of harvest?\n",
            "Model Response:\n",
            "To find out how many oranges they will have after 87 days of harvest, we need to multiply the number of sacks harvested per day by the number of days and then multiply by the number of oranges per sack.\n",
            "\n",
            "The number of sacks harvested per day is 66, and the number of days is 87. So we have:\n",
            "\n",
            "66 sacks/day * 87 days = 5732 sacks\n",
            "\n",
            "Each sack contains 25 oranges, so we have:\n",
            "\n",
            "5732 sacks * 25 oranges/sack = 143300 oranges\n",
            "\n",
            "So after 87 days of harvest, they will have 143300 oranges.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (61 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 2082, 12785, 5662, 1865, 220, 24, 36724, 13671, 323, 220, 19, 19] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 45 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    2888.15 ms /    45 tokens (   64.18 ms per token,    15.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =   22663.24 ms /    93 runs   (  243.69 ms per token,     4.10 tokens per second)\n",
            "llama_perf_context_print:       total time =   25726.61 ms /   138 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: An industrial machine made 9 shirts yesterday and 44 shirts today. It can make 3 shirts a minute. How many minutes did the machine work yesterday?\n",
            "Model Response:\n",
            "The machine made 9 shirts yesterday and 44 shirts today, so it made a total of 9 + 44 = 53 shirts.\n",
            "\n",
            "If the machine can make 3 shirts a minute, then it worked for 53 / 3 = 17.67 minutes.\n",
            "\n",
            "Since we can't have a fraction of a minute, we round up to the nearest whole number, so the machine worked for 18 minutes yesterday.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (69 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 33, 56758, 3867, 264, 1401, 518, 806, 6467, 323, 31847, 13, 1416] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 53 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    3433.09 ms /    53 tokens (   64.78 ms per token,    15.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =   13832.77 ms /    58 runs   (  238.50 ms per token,     4.19 tokens per second)\n",
            "llama_perf_context_print:       total time =   17371.68 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Bryan took a look at his books and magazines. If he has 9 books and 46 magazines in each of his 10 bookshelves How many magazines does he have in total?\n",
            "Model Response:\n",
            "Based on the information given, we can set up the following equation:\n",
            "\n",
            "10 * 9 + 46 = total magazines\n",
            "\n",
            "Solving for the total number of magazines:\n",
            "\n",
            "10 * 9 = 90 magazines\n",
            "\n",
            "So, Bryan has 90 magazines in total.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (55 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 55730, 1030, 220, 16, 22, 10500, 389, 806, 4540, 13, 4636, 7842] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 39 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    2522.07 ms /    39 tokens (   64.67 ms per token,    15.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =   13404.27 ms /    56 runs   (  239.36 ms per token,     4.18 tokens per second)\n",
            "llama_perf_context_print:       total time =   16029.22 ms /    95 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Dave had 17 apps on his phone. After adding some he had 18 left. How many apps did he add?\n",
            "Model Response:\n",
            "Based on the information given, Dave had 17 apps on his phone initially. After adding some, he had 18 left. So, the number of apps he added is:\n",
            "\n",
            "18 - 17 = 1\n",
            "\n",
            "So, Dave added 1 app.\n",
            "ChatGPT Verification: TRUE\n",
            "Tokenized input (64 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 38025, 702, 400, 220, 19, 13, 1260, 10788, 264, 31556, 3619, 369] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 48 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    5408.46 ms /    48 tokens (  112.68 ms per token,     8.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5291.99 ms /    24 runs   (  220.50 ms per token,     4.54 tokens per second)\n",
            "llama_perf_context_print:       total time =   10742.99 ms /    72 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Dan has $ 4. He bought a candy bar for $ 7 and a chocolate for $ 6. How much money did he spend buying the candy bar and chocolate?\n",
            "Model Response:\n",
            "Based on the information given, Dan spent $7 + $6 = $13 buying the candy bar and chocolate.\n",
            "ChatGPT Verification: TRUE\n",
            "Tokenized input (65 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 785, 67342, 304, 25179, 594, 4426, 525, 16645, 1119, 220, 16, 24] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 49 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    3137.44 ms /    49 tokens (   64.03 ms per token,    15.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =   20873.73 ms /    85 runs   (  245.57 ms per token,     4.07 tokens per second)\n",
            "llama_perf_context_print:       total time =   24172.81 ms /   134 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: The bananas in Philip's collection are organized into 196 groups. If there are a total of 392 bananas in Philip's banana collection How big is each group?\n",
            "Model Response:\n",
            "To find the size of each group, we need to divide the total number of bananas by the number of groups.\n",
            "\n",
            "The total number of bananas is 392, and there are 196 groups.\n",
            "\n",
            "So, we divide 392 by 196 to find the size of each group:\n",
            "\n",
            "392 / 196 = 2\n",
            "\n",
            "Therefore, each group contains 2 bananas.\n",
            "ChatGPT Verification: TRUE\n",
            "Tokenized input (71 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 785, 25385, 374, 220, 16, 17, 16, 23, 7541, 5538, 323, 807] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 4 prefix-match hit, remaining 54 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    3362.07 ms /    54 tokens (   62.26 ms per token,    16.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =   21054.75 ms /    86 runs   (  244.82 ms per token,     4.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   24578.54 ms /   140 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: The cave is 1218 feet deep and they are already at 849 feet. If they are travelling at speed of 17 How much farther until they reach the end of the cave?\n",
            "Model Response:\n",
            "To find out how much farther they are from the end of the cave, we need to calculate the remaining distance they have to travel.\n",
            "\n",
            "The cave is 1218 feet deep and they are already at 849 feet. So the remaining distance is 1218 - 849 = 369 feet.\n",
            "\n",
            "So they are 369 feet farther from the end of the cave.\n",
            "ChatGPT Verification: TRUE\n",
            "Tokenized input (61 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 785, 27942, 285, 13284, 304, 49752, 6118, 702, 220, 19, 16312, 13] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 4 prefix-match hit, remaining 44 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    3195.91 ms /    44 tokens (   72.63 ms per token,    13.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =   20321.84 ms /    86 runs   (  236.30 ms per token,     4.23 tokens per second)\n",
            "llama_perf_context_print:       total time =   23681.08 ms /   130 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: The Ferris wheel in paradise park has 4 seats. If 20 people can ride the wheel at the same time How many people can each seat hold?\n",
            "Model Response:\n",
            "To find out how many people each seat can hold, we need to divide the total number of people by the number of seats.\n",
            "\n",
            "The total number of people is 20, and the number of seats is 4.\n",
            "\n",
            "So, we divide 20 by 4 to get the number of people each seat can hold:\n",
            "\n",
            "20 ÷ 4 = 5\n",
            "\n",
            "Therefore, each seat can hold 5 people.\n",
            "ChatGPT Verification: TRUE\n",
            "Tokenized input (62 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 32, 36400, 1030, 220, 16, 22, 22, 40513, 323, 220, 16, 17] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 46 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    5553.97 ms /    46 tokens (  120.74 ms per token,     8.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =   26378.41 ms /   110 runs   (  239.80 ms per token,     4.17 tokens per second)\n",
            "llama_perf_context_print:       total time =   32145.23 ms /   156 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: A farmer had 177 tomatoes and 12 potatoes in his garden. If he picked 53 tomatoes How many tomatoes and potatoes does he have left?\n",
            "Model Response:\n",
            "Based on the information provided, we can set up the following equation to find out how many tomatoes and potatoes the farmer has left:\n",
            "\n",
            "177 tomatoes - 53 tomatoes = 12 potatoes\n",
            "\n",
            "So the farmer has 12 potatoes left. To find out how many tomatoes he has left, we can subtract the number of tomatoes he picked from the total number of tomatoes:\n",
            "\n",
            "177 tomatoes - 53 tomatoes = 124 tomatoes\n",
            "\n",
            "Therefore, the farmer has 124 tomatoes and 12 potatoes left.\n",
            "ChatGPT Verification: FALSE\n",
            "Tokenized input (67 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 97010, 51207, 16486, 18568, 13, 1260, 5558, 220, 21, 21, 16486, 18568] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 51 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    3304.04 ms /    51 tokens (   64.79 ms per token,    15.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4032.12 ms /    18 runs   (  224.01 ms per token,     4.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    7368.79 ms /    69 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Danny collects bottle caps. He lost 66 bottle caps at the park. Now he has 25 bottle caps in his collection. How many bottle caps did danny have at first?\n",
            "Model Response:\n",
            "Danny had 25 + 66 = 91 bottle caps at first.\n",
            "ChatGPT Verification: TRUE\n",
            "Tokenized input (63 tokens): [27, 91, 318, 4906, 91, 29, 872, 198, 32631, 3949, 220, 19, 14298, 304, 279, 6556, 11, 220, 20, 14298] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 47 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    6420.10 ms\n",
            "llama_perf_context_print: prompt eval time =    4186.99 ms /    47 tokens (   89.08 ms per token,    11.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10397.62 ms /    44 runs   (  236.31 ms per token,     4.23 tokens per second)\n",
            "llama_perf_context_print:       total time =   14663.14 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Jack received 4 emails in the morning, 5 emails in the afternoon and 8 emails in the evening. How many emails did Jack receive in the afternoon and evening?\n",
            "Model Response:\n",
            "Based on the information provided, Jack received 5 emails in the afternoon and 8 emails in the evening. So in total, he received 5 + 8 = 13 emails in the afternoon and evening.\n",
            "ChatGPT Verification: TRUE\n",
            "Time taken: 506.07s\n",
            "Accuracy on SVAMP dataset: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xAiO6FNQYZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}