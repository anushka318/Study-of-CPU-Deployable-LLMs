# Study-of-CPU-Deployable-LLMs

This project investigates the feasibility of running open-source LLMs efficiently on CPUs under varying hardware constraints. Students will simulate different CPU and RAM configurations using virtual machines, deploy various model architectures, and systematically measure latency. The goal is to analyze the relationship between compute resources (CPU, RAM) and model performance, identifying trade-offs and best practices for running LLMs on edge devices and resource-limited environments.
