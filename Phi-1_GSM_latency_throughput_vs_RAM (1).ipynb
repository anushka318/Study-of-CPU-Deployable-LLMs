{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e150c-c467-4579-8900-3af947f954d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import resource\n",
    "import psutil\n",
    "\n",
    "memory_limit = 16 * 1024 * 1024 * 1024 \n",
    "resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit))\n",
    "\n",
    "\n",
    "p = psutil.Process() \n",
    "p.cpu_affinity([0, 1, 2, 3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def limit_virtual_memory(max_mb):\n",
    "    soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "    resource.setrlimit(resource.RLIMIT_AS, (max_mb * 1024 * 1024, hard))\n",
    "\n",
    "limit_virtual_memory(1024*1024*1024*4)\n",
    "\n",
    "print(\"Physical GPU Devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "print(\"Logical GPU Devices:\", tf.config.list_logical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4efbf3-fee2-4c26-8f48-cb58c5f23ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import psutil\n",
    "#from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "#from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "#from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Disable GPU usage\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
    "\n",
    "\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "print(\"CPU Count:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234aee8-363a-4a8a-b3ec-fef885d5d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"t5-large\"  # You can use \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\" for larger versions\n",
    "#tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "#model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "#model_name = \"gpt2-medium\"  # You can use \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\" for larger versions\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "#model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "#model_name = \"facebook/bart-large\"\n",
    "#tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "#model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"Qwen/Qwen2-Math-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5646a-79ae-45e1-9df2-8f579a6d395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model.eval() \n",
    "\n",
    "\n",
    "prompt = \"What is the capital of Denmark?\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)  # Ensure padding is applied if needed\n",
    "\n",
    "\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd8375-db56-41cc-b2f9-64783f0d6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "questions = gsm8k[\"question\"][:3] \n",
    "\n",
    "\n",
    "def measure_latency(tokenizer, model, prompt, iterations=2):\n",
    "    latencies = []\n",
    "\n",
    "    chat_templated = f\"<|im_start|>user\\n{example['Body'], example['Question']}\\nLet's think step by step.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs[\"input_ids\"], \n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=50,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        latency = end_time - start_time\n",
    "        latencies.append(latency)\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Iteration {i+1}: {prompt} \\n  â†’ Model Output: {output_text}\\n\")\n",
    "\n",
    "    return sum(latencies) / len(latencies), latencies\n",
    "\n",
    "\n",
    "results = []\n",
    "for idx, question in enumerate(questions):\n",
    "    print(f\"\\nðŸ”¹ Question {idx+1}: {question}\")\n",
    "    avg_latency, latencies = measure_latency(tokenizer, model, question)\n",
    "    results.append((question, avg_latency, latencies))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx, (_, _, latencies) in enumerate(results):\n",
    "    plt.plot(latencies, label=f\"Q{idx+1}\")\n",
    "\n",
    "plt.title(\"Latency Over Iterations (GSM8K)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Latency (seconds)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee58b0-837a-41c8-a924-22451fdcd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8d94c-3b66-4922-9235-523ba37327b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "questions = gsm8k[\"question\"][:8] \n",
    "\n",
    "def measure_throughput_gsm(tokenizer, model, questions, batch_size=8, iterations=3):\n",
    "    throughputs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        batch = questions[i % len(questions): (i % len(questions)) + batch_size]  \n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, return_attention_mask=True)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,  # Ensures output length is controlled\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        throughput = batch_size / (end_time - start_time)\n",
    "        throughputs.append(throughput)\n",
    "\n",
    "    average_throughput = sum(throughputs) / len(throughputs)\n",
    "    return throughputs, average_throughput\n",
    "\n",
    "\n",
    "throughputs, average_throughput = measure_throughput_gsm(tokenizer, model, questions)\n",
    "\n",
    "print(f\"Average Throughput: {average_throughput:.2f} samples/second\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(throughputs)\n",
    "plt.title('Throughput Over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Throughput (samples/second)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U transformers bitsandbytes accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78559781",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5cce1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
