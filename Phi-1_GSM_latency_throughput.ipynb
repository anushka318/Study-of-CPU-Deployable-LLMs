{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18e150c-c467-4579-8900-3af947f954d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import resource\n",
    "import psutil\n",
    "\n",
    "# Set memory limit to 16GB (adjust if needed)\n",
    "memory_limit = 16 * 1024 * 1024 * 1024  # 16GB in bytes\n",
    "resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit))\n",
    "\n",
    "# Restrict CPU core usage\n",
    "p = psutil.Process()  # Get the current process\n",
    "p.cpu_affinity([0, 1, 2, 3]) # Restrict the process to use only CPU cores 0, 1, 2, and 3\n",
    "#p.cpu_affinity([0,1]) # Restrict the process to use only CPU cores 0, 1, 2, and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4efbf3-fee2-4c26-8f48-cb58c5f23ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import psutil\n",
    "#from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "#from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "#from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Disable GPU usage\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Optionally disable XLA devices to prevent further warnings\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
    "\n",
    "# Check available devices\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "# Print CPU Count\n",
    "print(\"CPU Count:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234aee8-363a-4a8a-b3ec-fef885d5d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"t5-large\"  # You can use \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\" for larger versions\n",
    "#tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "#model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "#model_name = \"gpt2-medium\"  # You can use \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\" for larger versions\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "#model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "#model_name = \"facebook/bart-large\"\n",
    "#tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "#model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"microsoft/phi-1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5646a-79ae-45e1-9df2-8f579a6d395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Set Pad Token and Model to Evaluation Mode\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as the pad_token\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"What is the capital of Denmark?\"\n",
    "\n",
    "# Encode the prompt (tokenize it) with padding\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)  # Ensure padding is applied if needed\n",
    "\n",
    "# Add attention mask to the inputs\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd8375-db56-41cc-b2f9-64783f0d6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load GSM8K dataset (test split)\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "questions = gsm8k[\"question\"][:3]  # Take the first 10 questions for benchmarking\n",
    "\n",
    "# Measure Latency\n",
    "def measure_latency(tokenizer, model, prompt, iterations=2):\n",
    "    latencies = []\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs[\"input_ids\"], \n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=50,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        latency = end_time - start_time\n",
    "        latencies.append(latency)\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Iteration {i+1}: {prompt} \\n  â†’ Model Output: {output_text}\\n\")\n",
    "\n",
    "    return sum(latencies) / len(latencies), latencies\n",
    "\n",
    "# Run latency tests on GSM8K\n",
    "results = []\n",
    "for idx, question in enumerate(questions):\n",
    "    print(f\"\\nðŸ”¹ Question {idx+1}: {question}\")\n",
    "    avg_latency, latencies = measure_latency(tokenizer, model, question)\n",
    "    results.append((question, avg_latency, latencies))\n",
    "\n",
    "# Plot Latency\n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx, (_, _, latencies) in enumerate(results):\n",
    "    plt.plot(latencies, label=f\"Q{idx+1}\")\n",
    "\n",
    "plt.title(\"Latency Over Iterations (GSM8K)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Latency (seconds)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee58b0-837a-41c8-a924-22451fdcd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8d94c-3b66-4922-9235-523ba37327b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load GSM8K dataset (test split)\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "questions = gsm8k[\"question\"][:3]  # Take the first 10 questions for benchmarking\n",
    "\n",
    "def measure_throughput_gsm(tokenizer, model, questions, batch_size=8, iterations=3):\n",
    "    throughputs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        batch = questions[i % len(questions): (i % len(questions)) + batch_size]  # Get batch of questions\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, return_attention_mask=True)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,  # Ensures output length is controlled\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        throughput = batch_size / (end_time - start_time)\n",
    "        throughputs.append(throughput)\n",
    "\n",
    "    average_throughput = sum(throughputs) / len(throughputs)\n",
    "    return throughputs, average_throughput\n",
    "\n",
    "# Run throughput measurement on GSM8K\n",
    "throughputs, average_throughput = measure_throughput_gsm(tokenizer, model, questions)\n",
    "\n",
    "print(f\"Average Throughput: {average_throughput:.2f} samples/second\")\n",
    "\n",
    "# Plot throughput over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(throughputs)\n",
    "plt.title('Throughput Over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Throughput (samples/second)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055408c9-9355-4885-acc5-ebfc29f903ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Track Memory Usage\n",
    "def get_memory_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    return memory.used  # Return memory used in bytes\n",
    "\n",
    "def track_memory_usage(model, inputs, attention_mask, iterations=25):\n",
    "    memory_usages = []\n",
    "    for _ in range(iterations):\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=attention_mask,  # Provide attention mask\n",
    "            max_length=100,  # Maximum number of tokens to generate\n",
    "            num_return_sequences=1,  # Number of output sequences\n",
    "            no_repeat_ngram_size=2,  # Prevent repetition\n",
    "            top_p=0.92,  # Nucleus sampling (controls randomness)\n",
    "            top_k=50,  # Top-k sampling\n",
    "            temperature=0.85,  # Lower temperature makes text less random\n",
    "            do_sample=True,  # Enable sampling mode to use top_p, top_k, and temperature\n",
    "            pad_token_id=tokenizer.pad_token_id  # Set pad token explicitly\n",
    "        )\n",
    "        memory_usage = get_memory_usage()\n",
    "        memory_usages.append(memory_usage)\n",
    "    \n",
    "    average_memory_usage = sum(memory_usages) / len(memory_usages)\n",
    "    return memory_usages, average_memory_usage\n",
    "\n",
    "# Define inputs and attention mask\n",
    "prompt = \"What is the capital of Denmark and what is the capital of India?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Track memory usage over iterations\n",
    "memory_usages, average_memory_usage = track_memory_usage(model, inputs, attention_mask)\n",
    "\n",
    "# Print average memory usage\n",
    "print(f\"Average Memory Usage: {average_memory_usage / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Plot memory usage over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([usage / (1024 * 1024) for usage in memory_usages])  # Convert bytes to MB for plotting\n",
    "plt.title('Memory Usage Over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cffe1-3f44-4e8c-8145-86c55ebd2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Latencies: \\n\", latencies)\n",
    "print(\"\\n\\nThroughputs: \\n\", throughputs)\n",
    "print(\"\\n\\nMemory Usages:\\n\", memory_usages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513d144-ac27-4ded-96d3-e6c1b3d18c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
