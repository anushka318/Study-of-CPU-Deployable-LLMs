{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d239d47-d304-49ee-b761-b149161c4bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Downloading openai-1.67.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.11/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading openai-1.67.0-py3-none-any.whl (580 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m580.2/580.2 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m351.8/351.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: typing-extensions, jiter, annotated-types, pydantic-core, pydantic, openai\n",
      "\u001b[33m  WARNING: The script openai is installed in '/home/pnemade/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 jiter-0.9.0 openai-1.67.0 pydantic-2.10.6 pydantic-core-2.27.2 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0ce294-f2c7-4518-ab4d-23dbdb0281e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (2.2.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall bitsandbytes -y\n",
    "!pip install --no-cache-dir bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4427ac56-6fb8-48d8-b9e6-88ed8363f27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.11/site-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (2.2.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18e150c-c467-4579-8900-3af947f954d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 01:17:07.142815: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-20 01:17:07.142916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-20 01:17:07.144795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-20 01:17:07.156199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "CPU Count: 128\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Disable GPU usage\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Optionally disable XLA devices to prevent further warnings\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
    "\n",
    "# Check available devices\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "# Print CPU Count\n",
    "print(\"CPU Count:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234aee8-363a-4a8a-b3ec-fef885d5d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# Load SVAMP dataset\n",
    "svamp = load_dataset(\"ChilleD/SVAMP\")\n",
    "\n",
    "# Quantization config to 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "# Load a suitable CoT-enabled model from Hugging Face\n",
    "model_name = \"Qwen/Qwen2-Math-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, device_map=\"cpu\", quantization_config=bnb_config)\n",
    "\n",
    "correct, total = 0, 5\n",
    "\n",
    "start_time = time.time()\n",
    "for example in svamp['test'].select(range(total)):\n",
    "    # Qwen-specific prompt\n",
    "    question_text = example['Body'] + \" \" + example['Question']\n",
    "    correct_answer = str(example['Answer'])\n",
    "    prompt = f\"<|im_start|>user\\n{question_text}\\nLet's think step by step.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    verification_prompt = f\"\"\"Question: {question_text} Model Response: {full_response} Correct Answer: {correct_answer} Based on the logical analysis of the Model Response and the Correct Answer provided, is the model's response correct? Reply ONLY with a single word: TRUE or FALSE.\"\"\"\n",
    "\n",
    "    chatgpt_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": verification_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    verification_result = chatgpt_response.choices[0].message.content.strip().upper()\n",
    "\n",
    "    print(f\"Question: {question_text}\")\n",
    "    print(f\"Model Response:\\n{full_response}\")\n",
    "    print(f\"ChatGPT Verification: {verification_result}\")\n",
    "\n",
    "    if verification_result == \"TRUE\":\n",
    "        correct += 1\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on SVAMP dataset: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd8375-db56-41cc-b2f9-64783f0d6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load GSM8K dataset (test split)\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "questions = gsm8k[\"question\"][:3]  # Take the first 10 questions for benchmarking\n",
    "\n",
    "# Measure Latency\n",
    "def measure_latency(tokenizer, model, prompt, iterations=3):\n",
    "    latencies = []\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs[\"input_ids\"], \n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=150,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        latency = end_time - start_time\n",
    "        latencies.append(latency)\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Iteration {i+1}: {prompt} \\n  ‚Üí Model Output: {output_text}\\n\")\n",
    "\n",
    "    return sum(latencies) / len(latencies), latencies\n",
    "\n",
    "# Run latency tests on GSM8K\n",
    "results = []\n",
    "for idx, question in enumerate(questions):\n",
    "    print(f\"\\nüîπ Question {idx+1}: {question}\")\n",
    "    avg_latency, latencies = measure_latency(tokenizer, model, question)\n",
    "    results.append((question, avg_latency, latencies))\n",
    "\n",
    "# Plot Latency\n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx, (_, _, latencies) in enumerate(results):\n",
    "    plt.plot(latencies, label=f\"Q{idx+1}\")\n",
    "\n",
    "plt.title(\"Latency Over Iterations (GSM8K)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Latency (seconds)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9a4dd-5cc6-4b87-a06a-49923169268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load GSM8K dataset (test split)\n",
    "#gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "#questions = gsm8k[\"question\"][:5]  # Take the first 10 questions for benchmarking\n",
    "\n",
    "#math_dataset = load_dataset(\"math\", \"main\", split=\"test\")\n",
    "#questions = math_dataset[\"question\"][:5]  # Take first 5 questions for benchmarking\n",
    "\n",
    "math_dataset = load_dataset(\"deepmind/mathematics\", \"arithmetic__mul\")\n",
    "questions = math_dataset[\"test\"][\"question\"][:3]  # First 5 test questions\n",
    "\n",
    "# Measure Latency\n",
    "def measure_latency(tokenizer, model, prompt, iterations=3):\n",
    "    latencies = []\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs[\"input_ids\"], \n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=50,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        latency = end_time - start_time\n",
    "        latencies.append(latency)\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Iteration {i+1}: {prompt} \\n  ‚Üí Model Output: {output_text}\\n\")\n",
    "\n",
    "    return sum(latencies) / len(latencies), latencies\n",
    "\n",
    "# Run latency tests on GSM8K\n",
    "results = []\n",
    "for idx, question in enumerate(questions):\n",
    "    print(f\"\\nüîπ Question {idx+1}: {question}\")\n",
    "    avg_latency, latencies = measure_latency(tokenizer, model, question)\n",
    "    results.append((question, avg_latency, latencies))\n",
    "\n",
    "# Plot Latency\n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx, (_, _, latencies) in enumerate(results):\n",
    "    plt.plot(latencies, label=f\"Q{idx+1}\")\n",
    "\n",
    "plt.title(\"Latency Over Iterations (GSM8K)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Latency (seconds)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee58b0-837a-41c8-a924-22451fdcd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8d94c-3b66-4922-9235-523ba37327b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load GSM8K dataset (test split)\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "questions = gsm8k[\"question\"][:3]  # Take the first 10 questions for benchmarking\n",
    "\n",
    "def measure_throughput_gsm(tokenizer, model, questions, batch_size=8, iterations=3):\n",
    "    tokenizer.padding_side = \"left\"  # Fix padding warning for decoder-only models\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Ensure pad token is set\n",
    "\n",
    "    throughputs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        batch = questions[i % len(questions): (i % len(questions)) + batch_size]  # Get batch of questions\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, return_attention_mask=True)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "    \n",
    "\n",
    "        start_time = time.time()\n",
    "        model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,  # Ensures output length is controlled\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        throughput = batch_size / (end_time - start_time)\n",
    "        throughputs.append(throughput)\n",
    "\n",
    "    average_throughput = sum(throughputs) / len(throughputs)\n",
    "    return throughputs, average_throughput\n",
    "\n",
    "# Run throughput measurement on GSM8K\n",
    "throughputs, average_throughput = measure_throughput_gsm(tokenizer, model, questions)\n",
    "\n",
    "print(f\"Average Throughput: {average_throughput:.2f} samples/second\")\n",
    "\n",
    "# Plot throughput over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(throughputs)\n",
    "plt.title('Throughput Over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Throughput (samples/second)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055408c9-9355-4885-acc5-ebfc29f903ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Track Memory Usage\n",
    "def get_memory_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    return memory.used  # Return memory used in bytes\n",
    "\n",
    "def track_memory_usage(model, inputs, attention_mask, iterations=25):\n",
    "    memory_usages = []\n",
    "    for _ in range(iterations):\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=attention_mask,  # Provide attention mask\n",
    "            max_length=100,  # Maximum number of tokens to generate\n",
    "            num_return_sequences=1,  # Number of output sequences\n",
    "            no_repeat_ngram_size=2,  # Prevent repetition\n",
    "            top_p=0.92,  # Nucleus sampling (controls randomness)\n",
    "            top_k=50,  # Top-k sampling\n",
    "            temperature=0.85,  # Lower temperature makes text less random\n",
    "            do_sample=True,  # Enable sampling mode to use top_p, top_k, and temperature\n",
    "            pad_token_id=tokenizer.pad_token_id  # Set pad token explicitly\n",
    "        )\n",
    "        memory_usage = get_memory_usage()\n",
    "        memory_usages.append(memory_usage)\n",
    "    \n",
    "    average_memory_usage = sum(memory_usages) / len(memory_usages)\n",
    "    return memory_usages, average_memory_usage\n",
    "\n",
    "# Define inputs and attention mask\n",
    "prompt = \"What is the capital of Denmark and what is the capital of India?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Track memory usage over iterations\n",
    "memory_usages, average_memory_usage = track_memory_usage(model, inputs, attention_mask)\n",
    "\n",
    "# Print average memory usage\n",
    "print(f\"Average Memory Usage: {average_memory_usage / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Plot memory usage over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([usage / (1024 * 1024) for usage in memory_usages])  # Convert bytes to MB for plotting\n",
    "plt.title('Memory Usage Over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cffe1-3f44-4e8c-8145-86c55ebd2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Latencies: \\n\", latencies)\n",
    "print(\"\\n\\nThroughputs: \\n\", throughputs)\n",
    "print(\"\\n\\nMemory Usages:\\n\", memory_usages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513d144-ac27-4ded-96d3-e6c1b3d18c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
